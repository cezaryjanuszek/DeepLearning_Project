{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from torch import empty, cat, arange\n",
    "from torch.nn.functional import fold, unfold\n",
    "from torch import nn\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading image data\n",
    "noisy_imgs_1, noisy_imgs_2 = torch.load('../../data/train_data.pkl')\n",
    "train_input, train_target = noisy_imgs_1.float()/255.0, noisy_imgs_2.float()/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take batch of size 1\n",
    "input = train_input[0].view(1, 3, 32, 32)\n",
    "target = train_target[0].view(1, 3, 32, 32)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.7781,  1.0103, -1.5367,  0.1919],\n",
       "           [-0.2604,  0.1187, -0.9530, -0.0493],\n",
       "           [ 0.2665,  0.1257,  2.2312,  0.8957],\n",
       "           [ 0.3404, -0.2406, -0.5551, -0.6570]],\n",
       " \n",
       "          [[-0.5045, -0.3335, -0.2546, -0.5980],\n",
       "           [-1.1061, -0.1766, -0.1381, -0.5813],\n",
       "           [ 0.1311, -0.5913,  1.4443, -1.0994],\n",
       "           [ 0.1172,  0.9564,  0.1378, -0.1556]],\n",
       " \n",
       "          [[ 0.7186, -0.8934, -0.3290, -0.8755],\n",
       "           [ 0.5194,  0.2253, -0.0102,  0.5849],\n",
       "           [ 1.7041, -1.7435, -1.2237, -0.8067],\n",
       "           [ 0.0472, -1.6299,  0.9207,  0.9559]]]]),\n",
       " tensor([[[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create some tensors for simple tests\n",
    "x = torch.randn((1, 3, 4, 4))\n",
    "\n",
    "# y = torch.ones(x.shape)\n",
    "y = torch.ones((1, 4, 3, 3))\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Conv2d Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing our conv2d layer\n",
    "my_conv = Conv2d(3, 4, 2)\n",
    "output = my_conv.forward(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_allclose(my_conv.forward(input), torch.nn.functional.conv2d(input, my_conv.weight, my_conv.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_424/583115890.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ml_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0ml_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = MSE()\n",
    "\n",
    "loss = criterion.forward(output, my_conv.forward(target))\n",
    "loss.requires_grad_()\n",
    "l_grad = loss.backward()\n",
    "l_grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_conv.input_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 31, 31])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_conv.weight.view(my_conv.output_channels, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 31, 31])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdoutput = (my_conv.weight.view(my_conv.output_channels, -1).t() @ output.view(1, my_conv.output_channels, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold(grdoutput, output_size=input.shape[2:], kernel_size=my_conv.kernel_size, padding=my_conv.padding, stride=my_conv.stride).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4, 4]), torch.Size([4]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_conv.input_x.shape, my_conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1133,  0.4086, -0.3195,  0.1317],\n",
       "          [-0.1134,  0.1100, -0.2876,  0.1214],\n",
       "          [ 0.4692, -0.6831,  1.4075, -0.0398],\n",
       "          [ 0.0674, -0.6124,  0.2772,  0.0436]],\n",
       "\n",
       "         [[-0.0764,  0.0615, -0.1131,  0.1226],\n",
       "          [-0.5011,  0.0688,  0.0460,  0.0334],\n",
       "          [-0.2054, -0.2830,  0.8174, -0.0631],\n",
       "          [-0.0350,  0.0097,  0.9152,  0.2967]],\n",
       "\n",
       "         [[ 0.2333, -0.1425, -0.0682,  0.0146],\n",
       "          [-0.0450,  0.4051, -0.2960,  0.3481],\n",
       "          [ 0.4077, -1.3182,  0.1713, -0.3866],\n",
       "          [ 0.0984, -0.5252,  0.5618,  0.1257]]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_conv.backward(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Upsampling (TransposeConv2d) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 64, 64])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_t_conv = Upsampling(3, 5, 2, stride=2)\n",
    "upsampled = my_t_conv.forward(input)\n",
    "upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_allclose(my_t_conv.forward(input), torch.nn.functional.conv_transpose2d(input, my_t_conv.weight, my_t_conv.bias, stride=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_t_conv.backward(upsampled).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.6125e-01,  3.4609e-01, -1.8182e-01,  4.0494e-01],\n",
       "          [ 2.5950e-01, -1.7082e-02, -7.9798e-01,  1.7086e-01],\n",
       "          [ 1.6125e-01,  1.8859e-01,  8.5036e-01,  2.9853e-01],\n",
       "          [ 2.5950e-01, -2.4566e-02, -6.0386e-01, -5.5168e-01]],\n",
       "\n",
       "         [[-1.7628e-01,  3.8483e-02,  6.5421e-01, -5.0083e-02],\n",
       "          [-2.7424e-01,  5.6687e-04,  3.3949e-01,  2.9070e-01],\n",
       "          [-1.7628e-01,  2.9196e-01,  8.8793e-01,  2.9944e-01],\n",
       "          [-2.7424e-01,  1.5897e-01, -2.5462e-01,  1.3513e-01]],\n",
       "\n",
       "         [[-1.1057e-01, -3.4472e-01, -2.6800e-01,  5.3072e-01],\n",
       "          [-2.8320e-02,  1.1625e-01,  5.6445e-01,  6.7065e-01],\n",
       "          [-1.1057e-01, -3.6104e-01, -8.0202e-02,  3.0841e-01],\n",
       "          [-2.8320e-02,  2.7095e-01,  8.4798e-01,  3.6433e-01]]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([Conv2d(3,4,2), ReLU()])\n",
    "forward_pass = model.forward(x)\n",
    "backward_pass = model.backward(forward_pass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.view(out_channels, -1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4, 4])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 9])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfolded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 9])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wxb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.7311, 0.5000],\n",
       "        [0.8808, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.7311]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu = ReLU()\n",
    "f=Sigmoid()\n",
    "\n",
    "f.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.2689, 0.5000],\n",
       "        [0.1192, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.2689]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05c62fd4a2c0b8f9b34fd07d525d94a1e9beb33a056c649ea73d5b364e4320b5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
