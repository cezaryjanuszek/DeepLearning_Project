{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model import *\n",
    "\n",
    "\n",
    "\n",
    "from torch import empty, cat, arange\n",
    "from torch.nn.functional import fold, unfold\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.2890, -0.3293, -3.3882, -1.2922],\n",
       "           [-0.1472, -1.4618,  0.8332, -0.6043],\n",
       "           [-0.5990,  0.3278,  0.5609,  1.5212],\n",
       "           [-0.5929, -1.3588, -1.9340, -0.0660]],\n",
       " \n",
       "          [[-0.7778,  0.0927, -0.2413, -0.9580],\n",
       "           [-0.2005, -0.7995,  1.7680,  1.2919],\n",
       "           [-0.4552,  1.6358, -0.0690,  0.2430],\n",
       "           [-1.1222,  0.0163, -0.1831, -0.4965]],\n",
       " \n",
       "          [[-0.6739,  1.0487,  2.0044,  0.1517],\n",
       "           [-0.8279, -0.4047, -0.9939,  1.2478],\n",
       "           [-0.5730,  0.6923, -0.3886, -0.5263],\n",
       "           [-0.4183, -2.1287,  0.2615,  2.5691]]]]),\n",
       " tensor([[[[1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1.]]]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((1, 3, 4, 4))\n",
    "\n",
    "# x = torch.tensor(\n",
    "    # [[0, 1., 0],     \n",
    "    # [2, 0, 0],\n",
    "    # [0, 0, 1]])\n",
    "\n",
    "# y = torch.ones(x.shape)\n",
    "y = torch.ones((1, 3, 4, 4))\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2417, -0.2417, -0.2417],\n",
       "          [-0.2417, -0.2417, -0.2417],\n",
       "          [-0.2417, -0.2417, -0.2417]],\n",
       "\n",
       "         [[ 0.2086,  0.2086,  0.2086],\n",
       "          [ 0.2086,  0.2086,  0.2086],\n",
       "          [ 0.2086,  0.2086,  0.2086]],\n",
       "\n",
       "         [[-0.2186, -0.2186, -0.2186],\n",
       "          [-0.2186, -0.2186, -0.2186],\n",
       "          [-0.2186, -0.2186, -0.2186]],\n",
       "\n",
       "         [[ 0.7873,  0.7873,  0.7873],\n",
       "          [ 0.7873,  0.7873,  0.7873],\n",
       "          [ 0.7873,  0.7873,  0.7873]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 4, 2)\n",
    "expected = conv(y)\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Unfold(kernel_size=2)(x)\n",
    "\n",
    "kernel_size = (2, 2)\n",
    "out_channels = 4\n",
    "\n",
    "# Output of convolution as a matrix product\n",
    "unfolded = torch.nn.functional.unfold(y, kernel_size=kernel_size)\n",
    "# unfolded\n",
    "\n",
    "wxb = conv.weight.view(out_channels, -1) @ unfolded + conv.bias.view(1, -1, 1)\n",
    "actual = wxb.view(1, out_channels, y.shape[2] - kernel_size[0] + 1, y.shape[3] - kernel_size[1]+ 1)\n",
    "torch.testing.assert_allclose(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 9])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conv.weight.view(out_channels, -1) @ unfolded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.view(out_channels, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4, 4])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 5, 5])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = (2,2)\n",
    "in_channels = 3\n",
    "out_channels = 4\n",
    "\n",
    "t_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)\n",
    "t_output = t_conv(y)\n",
    "t_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2, 2])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1543,  0.1625, -0.2144, -0.0718], requires_grad=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2160, -0.0289, -0.0289, -0.0289,  0.1871],\n",
       "         [ 0.0488,  0.3452,  0.3452,  0.3452,  0.2964],\n",
       "         [ 0.0488,  0.3452,  0.3452,  0.3452,  0.2964],\n",
       "         [ 0.0488,  0.3452,  0.3452,  0.3452,  0.2964],\n",
       "         [ 0.2648,  0.3741,  0.3741,  0.3741,  0.1093]],\n",
       "\n",
       "        [[ 0.1004,  0.3008,  0.3008,  0.3008,  0.2004],\n",
       "         [ 0.1111,  0.5863,  0.5863,  0.5863,  0.4752],\n",
       "         [ 0.1111,  0.5863,  0.5863,  0.5863,  0.4752],\n",
       "         [ 0.1111,  0.5863,  0.5863,  0.5863,  0.4752],\n",
       "         [ 0.0107,  0.2855,  0.2855,  0.2855,  0.2748]],\n",
       "\n",
       "        [[ 0.0116, -0.0736, -0.0736, -0.0736, -0.0853],\n",
       "         [ 0.0101,  0.4516,  0.4516,  0.4516,  0.4415],\n",
       "         [ 0.0101,  0.4516,  0.4516,  0.4516,  0.4415],\n",
       "         [ 0.0101,  0.4516,  0.4516,  0.4516,  0.4415],\n",
       "         [-0.0016,  0.5252,  0.5252,  0.5252,  0.5268]],\n",
       "\n",
       "        [[-0.0103,  0.0940,  0.0940,  0.0940,  0.1043],\n",
       "         [-0.0312,  0.4034,  0.4034,  0.4034,  0.4345],\n",
       "         [-0.0312,  0.4034,  0.4034,  0.4034,  0.4345],\n",
       "         [-0.0312,  0.4034,  0.4034,  0.4034,  0.4345],\n",
       "         [-0.0209,  0.3094,  0.3094,  0.3094,  0.3303]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folded = fold((t_conv.weight.view(in_channels, -1).t() @ y.view(in_channels, -1)), (y.shape[2]+kernel_size[0]-1,y.shape[3]+kernel_size[1]-1), kernel_size)\n",
    "folded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3704, -0.1833, -0.1833, -0.1833,  0.0327],\n",
       "          [-0.1055,  0.1909,  0.1909,  0.1909,  0.1420],\n",
       "          [-0.1055,  0.1909,  0.1909,  0.1909,  0.1420],\n",
       "          [-0.1055,  0.1909,  0.1909,  0.1909,  0.1420],\n",
       "          [ 0.1105,  0.2198,  0.2198,  0.2198, -0.0451]],\n",
       "\n",
       "         [[ 0.2629,  0.4633,  0.4633,  0.4633,  0.3629],\n",
       "          [ 0.2736,  0.7488,  0.7488,  0.7488,  0.6377],\n",
       "          [ 0.2736,  0.7488,  0.7488,  0.7488,  0.6377],\n",
       "          [ 0.2736,  0.7488,  0.7488,  0.7488,  0.6377],\n",
       "          [ 0.1732,  0.4480,  0.4480,  0.4480,  0.4373]],\n",
       "\n",
       "         [[-0.2027, -0.2880, -0.2880, -0.2880, -0.2996],\n",
       "          [-0.2043,  0.2372,  0.2372,  0.2372,  0.2272],\n",
       "          [-0.2043,  0.2372,  0.2372,  0.2372,  0.2272],\n",
       "          [-0.2043,  0.2372,  0.2372,  0.2372,  0.2272],\n",
       "          [-0.2159,  0.3109,  0.3109,  0.3109,  0.3124]],\n",
       "\n",
       "         [[-0.0821,  0.0222,  0.0222,  0.0222,  0.0325],\n",
       "          [-0.1030,  0.3316,  0.3316,  0.3316,  0.3627],\n",
       "          [-0.1030,  0.3316,  0.3316,  0.3316,  0.3627],\n",
       "          [-0.1030,  0.3316,  0.3316,  0.3316,  0.3627],\n",
       "          [-0.0927,  0.2376,  0.2376,  0.2376,  0.2585]]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folded + t_conv.bias.view(1, 4, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Conv2d Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing our conv2d layer\n",
    "my_conv = Conv2d(3, 4, 2)\n",
    "x2 = my_conv.forward(x)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_allclose(my_conv.forward(x), torch.nn.functional.conv2d(x, my_conv.weight, my_conv.bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our TransposeConv2d layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5, 5])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_t_conv = TransposeConv2d(3, 5, 2)\n",
    "y_expected = my_t_conv.forward(x)\n",
    "y_expected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_allclose(my_t_conv.forward(x), torch.nn.functional.conv_transpose2d(x, my_t_conv.weight, my_t_conv.bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.view(out_channels, -1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4, 4])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 9])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfolded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 9])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wxb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.7311, 0.5000],\n",
       "        [0.8808, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.7311]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu = ReLU()\n",
    "f=Sigmoid()\n",
    "\n",
    "f.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.2689, 0.5000],\n",
       "        [0.1192, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.2689]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05c62fd4a2c0b8f9b34fd07d525d94a1e9beb33a056c649ea73d5b364e4320b5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
